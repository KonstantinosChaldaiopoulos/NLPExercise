{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7245e97c",
   "metadata": {},
   "source": [
    "# Creation of a bigram model with a k-smoothing of 1 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a038dd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "from nltk.corpus import treebank\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11e108f",
   "metadata": {},
   "source": [
    "### Print file length ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8897b639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files=treebank.fileids()\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4efbe",
   "metadata": {},
   "source": [
    "### Print first sentence ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9afba57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.'], ['Mr.', 'Vinken', 'is', 'chairman', 'of', 'Elsevier', 'N.V.', ',', 'the', 'Dutch', 'publishing', 'group', '.']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank.sents(files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2155893e",
   "metadata": {},
   "source": [
    "### Split into training and test dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b159dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = treebank.fileids()[:170]\n",
    "test_files = treebank.fileids()[170:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b47f18",
   "metadata": {},
   "source": [
    "### Making sure that they are the correct length ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8a18e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(train_files))\n",
    "print(len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ebe7c3",
   "metadata": {},
   "source": [
    "### Create a vocab for words >3 ###\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "298afc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = Counter()\n",
    "\n",
    "for file in train_files:\n",
    "    for sent in treebank.sents(file):\n",
    "        token_counter.update([token.lower() for token in sent])\n",
    "\n",
    "unk_token = \"<UNK>\"\n",
    "vocab = {token.lower() for token, count in token_counter.items() if count >= 3}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3f5a45",
   "metadata": {},
   "source": [
    "### Creates a list of bigrams with boundary markers from sentences in training files ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a5d5be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigrams = []\n",
    "for file in train_files:\n",
    "    for sent in treebank.sents(file):\n",
    "        sent = ['<BOS>'] + [token.lower() if token.lower() in vocab else unk_token for token in sent] + ['<EOS>']\n",
    "        train_bigrams.extend(nltk.bigrams(sent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821a1a4d",
   "metadata": {},
   "source": [
    "### Computing smoothed bigram probabilities  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24c59f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0.01\n",
    "bigram_counts = defaultdict(Counter)\n",
    "for bigram in train_bigrams:\n",
    "    bigram_counts[bigram[0].lower()][bigram[1].lower()] += 1\n",
    "\n",
    "\n",
    "bigram_smoothed_probs = defaultdict(Counter)\n",
    "for w1 in bigram_counts:\n",
    "    total_count = sum(bigram_counts[w1].values()) + k * len(vocab)\n",
    "    for w2 in bigram_counts[w1]:\n",
    "        bigram_smoothed_probs[w1][w2] = (bigram_counts[w1][w2] + k) / total_count\n",
    "\n",
    "       \n",
    "test_bigrams = []\n",
    "test_bigram_count = 0\n",
    "for file in test_files:\n",
    "    for sent in treebank.sents(file):\n",
    "        sent = ['<BOS>'] + [token.lower() if token.lower() in vocab else unk_token for token in sent] + ['<EOS>'] # Replace with UNK also\n",
    "        test_bigrams.extend(nltk.bigrams(sent))\n",
    "        test_bigram_count += len(sent) \n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ca8a7",
   "metadata": {},
   "source": [
    "### Evaluating sum of ln prob ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d71cc39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_prob_sum=0.0        \n",
    "ln_prob_sum = 0.0\n",
    "\n",
    "for bigram in test_bigrams:\n",
    "    w1, w2 = bigram\n",
    "    w1_lower, w2_lower = w1.lower(), w2.lower()\n",
    "    prob = bigram_smoothed_probs[w1_lower][w2_lower] if w2_lower in bigram_smoothed_probs[w1_lower] else (k / (sum(bigram_counts[w1_lower].values()) + k * len(vocab)))\n",
    "    ln_prob_sum += math.log(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e2a43",
   "metadata": {},
   "source": [
    "### Print perplexity ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41b82f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119.64154057015432\n"
     ]
    }
   ],
   "source": [
    "perplexity = math.exp(-1 * (ln_prob_sum / test_bigram_count))\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5149ac",
   "metadata": {},
   "source": [
    "### Function to generate sentences based on 3 starting word of the model checking start with \\<BOS > ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "599ad52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(test_bigrams, bigram_model, start_word):\n",
    "    if ('<BOS>', start_word.lower()) not in [(bigram[0], bigram[1].lower()) for bigram in test_bigrams if bigram[0] == '<BOS>']:\n",
    "        raise ValueError(\"The provided start_word should be the second word of a bigram where the first word is '<BOS>' in the bigram model.\")\n",
    "    \n",
    "    sentence = [ start_word.lower()]\n",
    "    while sentence[-1] != '<eos>':\n",
    "        next_word_candidates = [word.lower() for word in list(bigram_model[sentence[-1]].keys())]\n",
    "        next_word_probs = list(bigram_model[sentence[-1]].values())\n",
    "        \n",
    "        if not next_word_candidates:\n",
    "            sentence.append('<EOS>')\n",
    "            break\n",
    "        \n",
    "        next_word = random.choices(next_word_candidates, next_word_probs)[0]\n",
    "        \n",
    "        if next_word == '<unk>':\n",
    "            continue\n",
    "        \n",
    "        sentence.append(next_word)\n",
    "    \n",
    "    return sentence[:-1] # exclude <EOS>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb571165",
   "metadata": {},
   "source": [
    "### Generate the sentences with starting words 'if', 'an, 'for' ###\n",
    "\n",
    "The sentences do not appear to convey any meaningful information or follow a coherent narrative or theme. Also the pereplexity seems to be higher for the lowercase model which indicates faulty code or small-unusual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6531acc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if after dealers ' report , merrill lynch , $ 130 million *u* ; 8 % 60 *u* for concern at which most english , dealers .\n",
      "an estimated 0 they could not a way to a joint venture .\n",
      "the chairman , giant montedison acquisition of *-4 to foreign country funds fell 20 % term as evidence of credit to veto at 98 *u* a leading political ties when mr. kaminski , comments did n't important as an american educators , vice minister for soviet union troubles , and homelessness is considering *-1 , with them -- like they `` something happen during a downturn reflects a few days ; iowa and white house 0 for the purchases *t*-1 .\n"
     ]
    }
   ],
   "source": [
    "start_words = ['if', 'an', 'the']\n",
    "generated_sentences = []\n",
    "\n",
    "for start_word in start_words:\n",
    "    generated_sentence = generate_sentence(test_bigrams, bigram_smoothed_probs, start_word.lower())\n",
    "    generated_sentences.append(generated_sentence)\n",
    "    print(' '.join(generated_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffda1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
