{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7245e97c",
   "metadata": {},
   "source": [
    "# Creation of a trigram model with a k-smoothing of 0.01 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a038dd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "from nltk.corpus import treebank\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11e108f",
   "metadata": {},
   "source": [
    "### Print file length ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8897b639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files=treebank.fileids()\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4efbe",
   "metadata": {},
   "source": [
    "### Print first sentence ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9afba57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.'], ['Mr.', 'Vinken', 'is', 'chairman', 'of', 'Elsevier', 'N.V.', ',', 'the', 'Dutch', 'publishing', 'group', '.']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank.sents(files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2155893e",
   "metadata": {},
   "source": [
    "### Split into training and test dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b159dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = treebank.fileids()[:170]\n",
    "test_files = treebank.fileids()[170:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b47f18",
   "metadata": {},
   "source": [
    "### Making sure that they are the correct length ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8a18e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(train_files))\n",
    "print(len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ebe7c3",
   "metadata": {},
   "source": [
    "### Create a vocab for words >3 ###\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "298afc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = Counter()\n",
    "\n",
    "for file in train_files:\n",
    "    for sent in treebank.sents(file):\n",
    "        token_counter.update([token.lower() for token in sent])\n",
    "        \n",
    "\n",
    "unk_token = \"<UNK>\"\n",
    "vocab = {token.lower() for token, count in token_counter.items() if count >= 3}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3f5a45",
   "metadata": {},
   "source": [
    "### Creates a list of bigrams with boundary markers from sentences in training files ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a5d5be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trigrams = []\n",
    "for file in train_files:\n",
    "    for sent in treebank.sents(file):\n",
    "        sent = ['<BOS>'] + [token.lower() if token.lower() in vocab else unk_token for token in sent] + ['<EOS>']\n",
    "        train_trigrams.extend(nltk.trigrams(sent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821a1a4d",
   "metadata": {},
   "source": [
    "### Computing smoothed bigram probabilities  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24c59f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "# Count trigrams in the training data\n",
    "trigram_counts = defaultdict(Counter)\n",
    "for trigram in train_trigrams:\n",
    "    trigram_counts[tuple(t.lower() for t in trigram[:-1])][trigram[-1].lower()] += 1\n",
    "\n",
    "# Calculate smoothed probabilities for trigrams\n",
    "trigram_smoothed_probs = defaultdict(Counter)\n",
    "for w1_w2 in trigram_counts:\n",
    "    total_count = sum(trigram_counts[w1_w2].values()) + k * len(vocab)\n",
    "    for w3 in trigram_counts[w1_w2]:\n",
    "        trigram_smoothed_probs[w1_w2][w3] = (trigram_counts[w1_w2][w3] + k) / total_count\n",
    "\n",
    "test_trigrams = []\n",
    "test_trigram_count = 0\n",
    "for file in test_files:\n",
    "    for sent in treebank.sents(file):\n",
    "        sent = ['<BOS>'] + [token.lower() if token.lower() in vocab else unk_token for token in sent] + ['<EOS>']\n",
    "        test_trigrams.extend(nltk.trigrams(sent))\n",
    "        test_trigram_count += len(sent) \n",
    "        \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ca8a7",
   "metadata": {},
   "source": [
    "### Evaluating sum of ln prob ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d71cc39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_prob_sum = 0.0\n",
    "ln_prob_sum = 0.0\n",
    "\n",
    "for trigram in test_trigrams:\n",
    "    w1, w2, w3 = trigram\n",
    "    w1_lower, w2_lower, w3_lower = w1.lower(), w2.lower(), w3.lower()\n",
    "\n",
    "    prob = trigram_smoothed_probs[(w1_lower, w2_lower)][w3_lower] if w3_lower in trigram_smoothed_probs[(w1_lower, w2_lower)] else (k / (sum(trigram_counts[(w1_lower, w2_lower)].values()) + k * len(vocab)))\n",
    "    ln_prob_sum += math.log(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e2a43",
   "metadata": {},
   "source": [
    "### Print perplexity ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41b82f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "857.1584334159471\n"
     ]
    }
   ],
   "source": [
    "perplexity = math.exp(-1 * (ln_prob_sum / test_trigram_count))\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5149ac",
   "metadata": {},
   "source": [
    "### Function to generate sentences based on 3 starting word of the model checking start with \\<BOS > ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "599ad52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(test_trigrams, trigram_smoothed_probs, start_word, max_length=60, unk_token='<unk>'):\n",
    "    generated_sentence = ['<bos>', start_word]\n",
    "    while len(generated_sentence) < max_length and generated_sentence[-1] != '<eos>':\n",
    "        w1, w2 = generated_sentence[-2], generated_sentence[-1]\n",
    "        next_word_candidates = list(trigram_smoothed_probs[(w1, w2)].keys())\n",
    "        next_word_candidates = [word for word in next_word_candidates if word != unk_token]  # Filter out <unk> token\n",
    "        \n",
    "        if next_word_candidates:\n",
    "            next_word_probs = [trigram_smoothed_probs[(w1, w2)][word] for word in next_word_candidates]\n",
    "            next_word = random.choices(next_word_candidates, weights=next_word_probs)[0]\n",
    "        else:\n",
    "            break  # If there are no valid next word candidates, stop generating the sentence\n",
    "        \n",
    "        generated_sentence.append(next_word)\n",
    "    \n",
    "    if generated_sentence[-1] == '<eos>':\n",
    "        generated_sentence.pop()  # Remove <EOS> token from the generated sentence\n",
    "    \n",
    "    return generated_sentence[1:]  # Exclude the <BOS> token from the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb571165",
   "metadata": {},
   "source": [
    "### Generate the sentences with starting words 'if', 'an, 'for' ###\n",
    "\n",
    "The generated sentences are better than bigram sentences,although not satisfying at all, but the trigram does not always find a possible next canditate so the use of break is needed to not create an infinite loop. Also the pereplexity seems to be higher for the lowercase model which indicates faulty code or small-unusual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6531acc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if president bush *?* , as members here always have *?* historically .\n",
      "an index of economic news had little effect on consumers , '' which he declined *-2 to be a power of excision over unconstitutional conditions in legislation that *t*-1 produced the ford\n",
      "for 10 years ago , nearly the same sounds that the japanese term for the military\n"
     ]
    }
   ],
   "source": [
    "start_words = ['if', 'an', 'for']\n",
    "generated_sentences = []\n",
    "\n",
    "for start_word in start_words:\n",
    "    generated_sentence = generate_sentence(test_trigrams, trigram_smoothed_probs, start_word)\n",
    "    generated_sentences.append(generated_sentence)\n",
    "    print(' '.join(generated_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b8c41e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
